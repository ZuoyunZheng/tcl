_base_: "default.yml"
data:
  batch_size: 1024
  dataset:
    train:
      - gcc3m
      - gcc12m

model:
  type: TCL
  clip_model: ViT-B/16  # NOTE only ViT-based backbones are supported.
  ie_freeze: 11  # index [1 ~ 12]
  ie_ignore_last_attn: true # use MaskCLIP
  masker:
    type: Masker
    decoder:
      type: GDecoder
      double: true
      n_layers: 2
      kernel_size: 3
      act: gelu
      norm: ln
    sim2mask:
      init_w: 10.0
      init_b: -2.5
      gumbel_tau: 1.0
      learnable: true

  # tcl
  tcl_w: 0.1
  area_w: 0.4

  # total variation
  tv_w: 1.0

train:
  total_steps: 120000
  ust_steps: 0
  base_lr: 4e-5
  weight_decay: 0.2
  min_lr: 1e-6
  clip_grad: 5.0
  warmup_steps: 15000
  # ust_steps: 30000  # train decoder only in this steps
  # base_lr: 3e-4
  # weight_decay: 0.05
  # min_lr: 4e-5
  # clip_grad: 5.0
  optimizer:
    eps: 1e-6

evaluate:
  pamr: true
  bg_thresh: 0.4
  kp_w: 0.3

  # eval_freq: 5000
  eval_freq: 5000
  template: custom
  task:
    - voc20
    # - t_context59
  cls:
    template: subset
  eval_only: true
  clip_cls: false
  clip_patch: false
  topk: 2
  threshold: true
  threshold_value: 0.0
  quantile: true
  quantile_value: 0.8
  tcl_mask_emb: true
  tcl_mask_img: false
  tcl_mask_refine: true

checkpoint:
  save_topk: 0

model_name: "release"
